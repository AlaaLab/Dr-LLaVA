<div align="center">
    <img src="assets/images/Dr-LLaVA.png" alt="Dr-LLaVA Logo" width="256px">
<p>Generated by <a href="https://openai.com/dall-e-3">DALL·E 3</a></p>
</div>

<div align="center">

<!-- # LLaVA-RLHF -->

# Dr-LLaVA: Visual Instruction Tuning with Symbolic Clinical Grounding

</div>

[[Project Page / Demo / Model Weights](https://llava-rlhf.github.io/)]

Code space for Dr-LLaVA, a conversational VLM finetuned for analyzing medical images. It is developed with the aim to reduce the inconstency across conversation for medical diagnosis.
We propose a new alignment algorithm that uses {\it symbolic representations} of clinical reasoning to ground VLMs in medical knowledge.


## 0. Setup

Please refer to [`llava_setup`](../llava_setup) for instructions on how to set up the customized llava package.

Additionally, you **should** run the following command to make sure the versions of some essential packages are correct:

```bash
pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118
pip install deepspeed==0.9.3
pip install peft==0.4.0
pip install transformers==4.31.0
pip install bitsandbytes==0.41.0
pip install datasets
```

**Note:** please install Pytorch 2.0.1 following the guidelines [here](https://pytorch.org/get-started/previous-versions/#v201). We found that the flash-attention implementation in the newest Pytorch Stable (2.1.0) could lead to buggy results. The codebase is tested with `torch==2.0.1+cu118`.

## 1. Built up the rule-based reward model 
## Train 
### Overview

Dr-LLaVA is trained on 4 A100 GPUs with 80GB memory. To train on fewer GPUs, you can reduce the `per_device_train_batch_size` and increase the `gradient_accumulation_steps` accordingly. Always keep the global batch size the same: `per_device_train_batch_size` x `gradient_accumulation_steps` x `num_gpus`.

This process involves in three steps. 
{\bf (i)} Construct the symbolic representations of clinical reasoning. 
These representations are utilized to {\bf (i)} generate GPT-4-guided visual instruction tuning data at scale, simulating clinician-VLM conversations with demonstrations of clinical reasoning, and {\bf (ii)} create an automatic reward function that evaluates the clinical validity of VLM generations throughout clinician-VLM interactions.


### Construct the symbolic representations of clinical reasoning
Given an question for a medical image, there are only a few options that the image shall response, and also unknown, given the current information we cannot draw any conclusion. 
Responses like this could be constructed into the categorical values which could be projected onto a logical graph tree. Just like the example below for blood maglicences. 
<div align="center">
    <img src="assets/images/Symbolic_representation.png" alt="Workflow" width="368px">
</div>
A medical image with a known diagnosis that could be observed from the image, must have other morlogical features that the clinician identified first prior to confirm the diagnosis. 
For example
A medical image to diagnosis framework could be conputalized into:
image -> Question about image description -> Question about image quality evaluation -> Question about morphogical feature 1 -> Question about the inference upon on the feature 1 -> ... -> Question about morphogical feature n -> Question about the inference upon on the feature n -> Diagnosis.

Starting with labeled medical images, we use symbolic representations of clinical reasoning and GPT-4 to generate realistic conversations between a VLM and a clinician about the visual content of each image. 
These multi-turn conversations are designed to reflect various styles of clinician-VLM interactions, with each conversation comprising a sequence of related questions that demonstrate accurate clinical reasoning. 

#### Construct your own clinical logics for medical diagnosis

Here is an example of the clinical logics in addition to our private dataset. Taking X-ray image as an example. 

You can find more examples and details in the [Example_Clinical_Logics.md](assets/Example_Clinical_Logics.md) file.

After which you can agument the QAs with large language model available to increase the diversity of the QAs. 


### Using clinical logics to construct the reward model

### Supervised finetuning the LLaVA model

Note, the RL part of the model will not work unless the supervised fintuning is performed. 

After curated the dataset and store the training and test data in the LLaVA.josn format. 
Run 
```
bash scripts/7b-v1.5-224/initialize_policy_model.sh
```

### Training the RL Model with PPO
Run
```
bash scripts/7b-v1.5-224/train_rl_model.sh
```
## Examples

<div align="center">
    <img src="assets/images/example.png" alt="Example" width="768px">
</div>



## Citations

If you find this repo useful for your research, please consider citing the paper

Dr-LLAVA：
```bibtex
@article{sun2024dr,
  title={Dr-LLaVA: Visual Instruction Tuning with Symbolic Clinical Grounding},
  author={Sun, Shenghuan and Goldgof, Gregory M and Schubert, Alexander and Sun, Zhiqing and Hartvigsen, Thomas and Butte, Atul J and Alaa, Ahmed},
  journal={arXiv preprint arXiv:2405.19567},
  year={2024}
}

```
LLaVA-RLHF:

```bibtex
@article{sun2023aligning,
  title={Aligning large multimodal models with factually augmented rlhf},
  author={Sun, Zhiqing and Shen, Sheng and Cao, Shengcao and Liu, Haotian and Li, Chunyuan and Shen, Yikang and Gan, Chuang and Gui, Liang-Yan and Wang, Yu-Xiong and Yang, Yiming and others},
  journal={arXiv preprint arXiv:2309.14525},
  year={2023}
}
```

LLaVA:

```bibtex
@misc{liu2023llava,
      title={Visual Instruction Tuning},
      author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
      publisher={arXiv:2304.08485},
      year={2023},
}
```

SALMON:

```bibtex
@article{sun2023salmon,
  title={SALMON: Self-Alignment with Principle-Following Reward Models},
  author={Sun, Zhiqing and Shen, Yikang and Zhang, Hongxin and Zhou, Qinhong and Chen, Zhenfang and Cox, David and Yang, Yiming and Gan, Chuang},
  journal={arXiv preprint arXiv:2310.05910},
  year={2023}
}
```

## Acknowledgements

We thank [Meta LLaMA team](https://github.com/facebookresearch/llama), [Standford Alpaca team](https://github.com/tatsu-lab/stanford_alpaca), [Vicuna team](https://github.com/lm-sys/FastChat), [LLaVA team](https://github.com/haotian-liu/LLaVA), [QLoRA team](https://github.com/artidoro/qlora), [Hugging Face PEFT](https://github.com/huggingface/peft), and [AlpacaFarm team](https://github.com/tatsu-lab/alpaca_farm) for their open-source efforts in democratizing large language models.
