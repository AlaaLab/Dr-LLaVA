<div align="center">
    <img src="assets/images/Dr-LLaVA.png" alt="Dr-LLaVA Logo" width="256px">
<p>Generated by <a href="https://openai.com/dall-e-3">DALLÂ·E 3</a></p>
</div>

<div align="center">

<!-- # LLaVA-RLHF -->

# Dr-LLaVA: Visual Instruction Tuning with Symbolic Clinical Grounding

</div>

[[Project Page / Demo / Model Weights](https://llava-rlhf.github.io/)]

Code space for Dr-LLaVA, a conversational VLM finetuned for analyzing medical images. It is developed with the aim to reduce the inconstency across conversation for medical diagnosis.
We propose a new alignment algorithm that uses {\it symbolic representations} of clinical reasoning to ground VLMs in medical knowledge.


## Train 
### Overview

Dr-LLaVA is trained on 4 A100 GPUs with 80GB memory. To train on fewer GPUs, you can reduce the `per_device_train_batch_size` and increase the `gradient_accumulation_steps` accordingly. Always keep the global batch size the same: `per_device_train_batch_size` x `gradient_accumulation_steps` x `num_gpus`.

This process involves in three steps. 
{\bf (i)} Construct the symbolic representations of clinical reasoning. 
These representations are utilized to {\bf (i)} generate GPT-4-guided visual instruction tuning data at scale, simulating clinician-VLM conversations with demonstrations of clinical reasoning, and {\bf (ii)} create an automatic reward function that evaluates the clinical validity of VLM generations throughout clinician-VLM interactions.


### Construct the symbolic representations of clinical reasoning

<div align="center">
    <img src="assets/images/Symbolic_representation.pdf" alt="Workflow" width="768px">
</div>






## Examples

<div align="center">
    <img src="https://llava-rlhf.github.io/LLaVA_files/cmp_moto.jpg" alt="Example 1" width="768px">
</div>

<div align="center">
    <img src="https://llava-rlhf.github.io/LLaVA_files/cmp_airport.jpg" alt="Example 2" width="768px">
</div>

## Citations

If you find this repo useful for your research, please consider citing the paper

LLaVA-RLHF:

```bibtex
@article{sun2023aligning,
  title={Aligning large multimodal models with factually augmented rlhf},
  author={Sun, Zhiqing and Shen, Sheng and Cao, Shengcao and Liu, Haotian and Li, Chunyuan and Shen, Yikang and Gan, Chuang and Gui, Liang-Yan and Wang, Yu-Xiong and Yang, Yiming and others},
  journal={arXiv preprint arXiv:2309.14525},
  year={2023}
}
```

LLaVA:

```bibtex
@misc{liu2023llava,
      title={Visual Instruction Tuning},
      author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
      publisher={arXiv:2304.08485},
      year={2023},
}
```

SALMON:

```bibtex
@article{sun2023salmon,
  title={SALMON: Self-Alignment with Principle-Following Reward Models},
  author={Sun, Zhiqing and Shen, Yikang and Zhang, Hongxin and Zhou, Qinhong and Chen, Zhenfang and Cox, David and Yang, Yiming and Gan, Chuang},
  journal={arXiv preprint arXiv:2310.05910},
  year={2023}
}
```

## Acknowledgements

We thank [Meta LLaMA team](https://github.com/facebookresearch/llama), [Standford Alpaca team](https://github.com/tatsu-lab/stanford_alpaca), [Vicuna team](https://github.com/lm-sys/FastChat), [LLaVA team](https://github.com/haotian-liu/LLaVA), [QLoRA team](https://github.com/artidoro/qlora), [Hugging Face PEFT](https://github.com/huggingface/peft), and [AlpacaFarm team](https://github.com/tatsu-lab/alpaca_farm) for their open-source efforts in democratizing large language models.
