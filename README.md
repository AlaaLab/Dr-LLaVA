<div align="center">
    <img src="assets/images/Dr-LLaVA.png" alt="Dr-LLaVA Logo" width="256px">
<p>Generated by <a href="https://openai.com/dall-e-3">DALL·E 3</a></p>
</div>

<div align="center">

<!-- # LLaVA-RLHF -->

# Dr-LLaVA: Visual Instruction Tuning with Symbolic Clinical Grounding

</div>

[[Project Page / Demo / Model Weights](https://llava-rlhf.github.io/)]

Code space for Dr-LLaVA, a conversational VLM finetuned for analyzing medical images. It is developed with the aim to reduce the inconstency across conversation for medical diagnosis.
We propose a new alignment algorithm that uses {\it symbolic representations} of clinical reasoning to ground VLMs in medical knowledge.


## Train 
### Overview

Dr-LLaVA is trained on 4 A100 GPUs with 80GB memory. To train on fewer GPUs, you can reduce the `per_device_train_batch_size` and increase the `gradient_accumulation_steps` accordingly. Always keep the global batch size the same: `per_device_train_batch_size` x `gradient_accumulation_steps` x `num_gpus`.

This process involves in three steps. 
{\bf (i)} Construct the symbolic representations of clinical reasoning. 
These representations are utilized to {\bf (i)} generate GPT-4-guided visual instruction tuning data at scale, simulating clinician-VLM conversations with demonstrations of clinical reasoning, and {\bf (ii)} create an automatic reward function that evaluates the clinical validity of VLM generations throughout clinician-VLM interactions.


### Construct the symbolic representations of clinical reasoning

<div align="center">
    <img src="assets/images/Symbolic_representation.png" alt="Workflow" width="368px">
</div>
Starting with labeled medical images, we use symbolic representations of clinical reasoning and GPT-4 to generate realistic conversations between a VLM and a clinician about the visual content of each image. 
These multi-turn conversations are designed to reflect various styles of clinician-VLM interactions, with each conversation comprising a sequence of related questions that demonstrate accurate clinical reasoning. 


Given an question for a medical image, there are only a few options that the image shall response, and also unknown, given the current information we cannot draw any conclusion. 
Responses like this could be constructed into the categorical values which could be projected onto a logical graph tree. Just like the example below for blood maglicences. 



## Examples

<div align="center">
    <img src="assets/images/example.png" alt="Example" width="768px">
</div>



## Citations

If you find this repo useful for your research, please consider citing the paper

Dr-LLAVA：
```bibtex
@article{sun2024dr,
  title={Dr-LLaVA: Visual Instruction Tuning with Symbolic Clinical Grounding},
  author={Sun, Shenghuan and Goldgof, Gregory M and Schubert, Alexander and Sun, Zhiqing and Hartvigsen, Thomas and Butte, Atul J and Alaa, Ahmed},
  journal={arXiv preprint arXiv:2405.19567},
  year={2024}
}

```
LLaVA-RLHF:

```bibtex
@article{sun2023aligning,
  title={Aligning large multimodal models with factually augmented rlhf},
  author={Sun, Zhiqing and Shen, Sheng and Cao, Shengcao and Liu, Haotian and Li, Chunyuan and Shen, Yikang and Gan, Chuang and Gui, Liang-Yan and Wang, Yu-Xiong and Yang, Yiming and others},
  journal={arXiv preprint arXiv:2309.14525},
  year={2023}
}
```

LLaVA:

```bibtex
@misc{liu2023llava,
      title={Visual Instruction Tuning},
      author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
      publisher={arXiv:2304.08485},
      year={2023},
}
```

SALMON:

```bibtex
@article{sun2023salmon,
  title={SALMON: Self-Alignment with Principle-Following Reward Models},
  author={Sun, Zhiqing and Shen, Yikang and Zhang, Hongxin and Zhou, Qinhong and Chen, Zhenfang and Cox, David and Yang, Yiming and Gan, Chuang},
  journal={arXiv preprint arXiv:2310.05910},
  year={2023}
}
```

## Acknowledgements

We thank [Meta LLaMA team](https://github.com/facebookresearch/llama), [Standford Alpaca team](https://github.com/tatsu-lab/stanford_alpaca), [Vicuna team](https://github.com/lm-sys/FastChat), [LLaVA team](https://github.com/haotian-liu/LLaVA), [QLoRA team](https://github.com/artidoro/qlora), [Hugging Face PEFT](https://github.com/huggingface/peft), and [AlpacaFarm team](https://github.com/tatsu-lab/alpaca_farm) for their open-source efforts in democratizing large language models.
