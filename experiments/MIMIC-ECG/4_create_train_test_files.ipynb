{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to filter data for Dr-LLaVA Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ecg_df = pd.read_csv('../../data/mimic-acute-mi.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>ecg_time</th>\n",
       "      <th>st_elevation</th>\n",
       "      <th>st_depression</th>\n",
       "      <th>t_wave</th>\n",
       "      <th>Acute_MI</th>\n",
       "      <th>study_id</th>\n",
       "      <th>text</th>\n",
       "      <th>valuenum</th>\n",
       "      <th>comments</th>\n",
       "      <th>troponin</th>\n",
       "      <th>STEMI</th>\n",
       "      <th>NSTEMI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29196424.0</td>\n",
       "      <td>2087-05-08 00:05:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44673611</td>\n",
       "      <td>Allergies: \\nlisinopril / vancomycin\\n \\nChief...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21091437.0</td>\n",
       "      <td>2087-05-08 00:05:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43819938</td>\n",
       "      <td>Allergies: \\namitriptyline / Sulfa (Sulfonamid...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>cTropnT &gt; 0.10 ng/mL suggests Acute MI.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21236438.0</td>\n",
       "      <td>2088-04-14 02:58:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47239325</td>\n",
       "      <td>Allergies: \\nMULTIPLE - SEE LIST **** / amoxic...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>cTropnT &gt; 0.10 ng/mL suggests Acute MI.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23821411.0</td>\n",
       "      <td>2089-12-11 00:06:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41010651</td>\n",
       "      <td>Allergies: \\nchlorhexidine\\n \\nChief Complaint...</td>\n",
       "      <td>0.07</td>\n",
       "      <td>cTropnT &gt; 0.10 ng/mL suggests Acute MI.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28274927.0</td>\n",
       "      <td>2090-08-21 00:12:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49791716</td>\n",
       "      <td>Allergies: \\nNo Known Allergies / Adverse Drug...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      hadm_id             ecg_time  st_elevation  st_depression  t_wave  \\\n",
       "0  29196424.0  2087-05-08 00:05:00             0              0       0   \n",
       "1  21091437.0  2087-05-08 00:05:00             0              0       0   \n",
       "3  21236438.0  2088-04-14 02:58:00             0              0       0   \n",
       "4  23821411.0  2089-12-11 00:06:00             0              0       0   \n",
       "5  28274927.0  2090-08-21 00:12:00             0              0       0   \n",
       "\n",
       "   Acute_MI  study_id                                               text  \\\n",
       "0         0  44673611  Allergies: \\nlisinopril / vancomycin\\n \\nChief...   \n",
       "1         0  43819938  Allergies: \\namitriptyline / Sulfa (Sulfonamid...   \n",
       "3         0  47239325  Allergies: \\nMULTIPLE - SEE LIST **** / amoxic...   \n",
       "4         0  41010651  Allergies: \\nchlorhexidine\\n \\nChief Complaint...   \n",
       "5         0  49791716  Allergies: \\nNo Known Allergies / Adverse Drug...   \n",
       "\n",
       "   valuenum                                 comments  troponin  STEMI  NSTEMI  \n",
       "0       NaN                                      NaN         0      0       0  \n",
       "1      0.03  cTropnT > 0.10 ng/mL suggests Acute MI.         1      0       0  \n",
       "3      0.02  cTropnT > 0.10 ng/mL suggests Acute MI.         1      0       0  \n",
       "4      0.07  cTropnT > 0.10 ng/mL suggests Acute MI.         1      0       0  \n",
       "5       NaN                                      NaN         0      0       0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(272109, 13)\n"
     ]
    }
   ],
   "source": [
    "all_ecg_df = all_ecg_df[~all_ecg_df['text'].isna()]\n",
    "all_ecg_df = all_ecg_df[(all_ecg_df['troponin'] & ~all_ecg_df['valuenum'].isnull() & ~all_ecg_df['comments'].isnull()) | (all_ecg_df['troponin'] == 0)]\n",
    "display(all_ecg_df.head())\n",
    "print(all_ecg_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8482\n",
      "30144\n",
      "\n",
      "31925\n",
      "1572\n",
      "493\n"
     ]
    }
   ],
   "source": [
    "print(all_ecg_df.STEMI.sum())\n",
    "print(all_ecg_df.NSTEMI.sum())\n",
    "print()\n",
    "\n",
    "print(all_ecg_df.st_elevation.sum())\n",
    "print(all_ecg_df.st_depression.sum())\n",
    "print(all_ecg_df.t_wave.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampled DataFrame Shape: (30000, 13)\n",
      "STEMI Count: 2421\n",
      "NSTEMI Count: 5547\n",
      "ST Elevation Count: 6969\n",
      "ST Depression Count: 707\n",
      "T-wave Inversion Count: 354\n"
     ]
    }
   ],
   "source": [
    "# Function to sample rows for a specific condition\n",
    "def sample_condition(df, condition, count, sampled_indices):\n",
    "    condition_df = df[condition & ~df.index.isin(sampled_indices)]\n",
    "    if len(condition_df) < count:\n",
    "        raise ValueError(f\"Not enough rows to sample for condition: {condition}\")\n",
    "    sampled = condition_df.sample(n=count, random_state=RANDOM_STATE)\n",
    "    sampled_dfs.append(sampled)\n",
    "    return sampled.index\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Define the minimum required counts for each ECG category\n",
    "required_counts = {\n",
    "    'STEMI': 900,\n",
    "    'NSTEMI': 3200,\n",
    "    'ST_elevation': 3500,\n",
    "    'ST_depression': 500,\n",
    "    'T_wave_inversion': 300\n",
    "}\n",
    "\n",
    "# Create a copy of the original dataframe to work with\n",
    "df = all_ecg_df.copy()\n",
    "\n",
    "# Initialize an empty list to store the sampled dataframes\n",
    "sampled_dfs = []\n",
    "\n",
    "# Initialize a set to keep track of sampled indices to avoid duplication where necessary\n",
    "sampled_indices = set()\n",
    "\n",
    "# Sample STEMI ECGs\n",
    "sampled_indices.update(sample_condition(\n",
    "    df,\n",
    "    df['STEMI'] == 1,\n",
    "    required_counts['STEMI'],\n",
    "    sampled_indices\n",
    "))\n",
    "\n",
    "# Sample NSTEMI ECGs\n",
    "sampled_indices.update(sample_condition(\n",
    "    df,\n",
    "    df['NSTEMI'] == 1,\n",
    "    required_counts['NSTEMI'],\n",
    "    sampled_indices\n",
    "))\n",
    "\n",
    "# Sample ST-elevation ECGs\n",
    "sampled_indices.update(sample_condition(\n",
    "    df,\n",
    "    df['st_elevation'] == 1,\n",
    "    required_counts['ST_elevation'],\n",
    "    sampled_indices\n",
    "))\n",
    "\n",
    "# Sample ST-depression ECGs\n",
    "sampled_indices.update(sample_condition(\n",
    "    df,\n",
    "    df['st_depression'] == 1,\n",
    "    required_counts['ST_depression'],\n",
    "    sampled_indices\n",
    "))\n",
    "\n",
    "# Sample T-wave inversion ECGs\n",
    "sampled_indices.update(sample_condition(\n",
    "    df,\n",
    "    df['t_wave'] == 1,\n",
    "    required_counts['T_wave_inversion'],\n",
    "    sampled_indices\n",
    "))\n",
    "\n",
    "# Concatenate all sampled dataframes\n",
    "downsampled_df = pd.concat(sampled_dfs)\n",
    "\n",
    "# Calculate the remaining number of rows to reach 30,000\n",
    "remaining_rows = 30000 - len(downsampled_df)\n",
    "\n",
    "# Check if there are enough remaining rows to sample\n",
    "if remaining_rows > 0:\n",
    "    # Exclude already sampled indices\n",
    "    remaining_df = df[~df.index.isin(sampled_indices)]\n",
    "    \n",
    "    if len(remaining_df) < remaining_rows:\n",
    "        raise ValueError(\"Not enough remaining rows to reach 30,000 after sampling required categories.\")\n",
    "    \n",
    "    # Sample the remaining rows\n",
    "    remaining_sampled = remaining_df.sample(n=remaining_rows, random_state=RANDOM_STATE)\n",
    "    downsampled_df = pd.concat([downsampled_df, remaining_sampled])\n",
    "\n",
    "# Shuffle the final dataframe\n",
    "downsampled_df = downsampled_df.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "# Optional: Verify the counts\n",
    "print(\"Downsampled DataFrame Shape:\", downsampled_df.shape)\n",
    "print(\"STEMI Count:\", downsampled_df['STEMI'].sum())\n",
    "print(\"NSTEMI Count:\", downsampled_df['NSTEMI'].sum())\n",
    "print(\"ST Elevation Count:\", (downsampled_df['st_elevation'] == 1).sum())\n",
    "print(\"ST Depression Count:\", (downsampled_df['st_depression'] == 1).sum())\n",
    "print(\"T-wave Inversion Count:\", (downsampled_df['t_wave'] == 1).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49931631, 47699806, 45604321, 49910525, 40829195, 48006520, 48573578, 48742280, 49282801, 49289007]\n",
      "Total number of study_ids: 30000\n",
      "Number of study_ids with corresponding image files: 29965\n",
      "Percentage matched: 99.88%\n"
     ]
    }
   ],
   "source": [
    "study_ids = downsampled_df['study_id'].tolist()\n",
    "print(study_ids[:10])\n",
    "# Path to the image directory\n",
    "image_dir = '../../data/image_folder'\n",
    "\n",
    "# Initialize a set to store the integer filenames\n",
    "image_ids = set()\n",
    "\n",
    "# Traverse through each file in the image directory\n",
    "for filename in os.listdir(image_dir):\n",
    "    # Check if the file is a JPEG image\n",
    "    if filename.lower().endswith('.jpeg'):\n",
    "        # Remove the '.jpeg' extension\n",
    "        name_without_ext = filename[:-5]\n",
    "        try:\n",
    "            # Convert the filename to an integer and add to the set\n",
    "            file_id = int(name_without_ext)\n",
    "            image_ids.add(file_id)\n",
    "        except ValueError:\n",
    "            # If the filename is not an integer, skip it\n",
    "            print(f\"Skipping file with non-integer name: {filename}\")\n",
    "\n",
    "# Convert study_ids to a set for efficient lookup\n",
    "study_ids_set = set(study_ids)\n",
    "\n",
    "# Find the intersection of study_ids and image_ids\n",
    "matching_ids = study_ids_set.intersection(image_ids)\n",
    "\n",
    "# Calculate the number of matching IDs\n",
    "num_matching = len(matching_ids)\n",
    "total_study_ids = len(study_ids)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Total number of study_ids: {total_study_ids}\")\n",
    "print(f\"Number of study_ids with corresponding image files: {num_matching}\")\n",
    "print(f\"Percentage matched: { (num_matching / total_study_ids) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29965, 13)\n"
     ]
    }
   ],
   "source": [
    "downsampled_df = downsampled_df[downsampled_df['study_id'].isin(matching_ids)] \n",
    "print(downsampled_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_df.to_csv('../../data/mimic-acute-mi_modelling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/conversations_new.json') as f:\n",
    "    conversations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/272109 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272109/272109 [03:42<00:00, 1223.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conversations after filtering: 29965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "study_id_list = downsampled_df['study_id'].tolist()\n",
    "filtered_conversations = [item for item in tqdm(conversations) if int(item['id']) in downsampled_df['study_id'].tolist()]\n",
    "print(f\"Number of conversations after filtering: {len(filtered_conversations)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/conversations_modelling.json', 'w') as f:\n",
    "    json.dump(filtered_conversations, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training study_ids: 23972\n",
      "Testing study_ids: 5993\n",
      "Training DataFrame shape: (23972, 13)\n",
      "Test DataFrame shape: (5993, 13)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training conversations count: 23972\n",
      "Test conversations count: 5993\n",
      "Training and test DataFrames saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "RANDOM_STATE = 42\n",
    "TRAIN_RATIO = 0.8\n",
    "\n",
    "# Paths to JSON files\n",
    "train_json_path = '../../data/train_conversations.json'\n",
    "test_json_path = '../../data/test_conversations.json'\n",
    "\n",
    "# Paths to save split DataFrames\n",
    "train_df_path = '../../data/train_downsampled_df.csv'\n",
    "test_df_path = '../../data/test_downsampled_df.csv'\n",
    "\n",
    "\n",
    "# 1. Split the study_ids\n",
    "study_ids = downsampled_df['study_id'].astype(int).unique()\n",
    "train_ids, test_ids = train_test_split(\n",
    "    study_ids,\n",
    "    test_size=1 - TRAIN_RATIO,\n",
    "    random_state=RANDOM_STATE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Training study_ids: {len(train_ids)}\")\n",
    "print(f\"Testing study_ids: {len(test_ids)}\")\n",
    "\n",
    "# 2. Create training and test DataFrames\n",
    "train_df = downsampled_df[downsampled_df['study_id'].astype(int).isin(train_ids)].reset_index(drop=True)\n",
    "test_df = downsampled_df[downsampled_df['study_id'].astype(int).isin(test_ids)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Training DataFrame shape: {train_df.shape}\")\n",
    "print(f\"Test DataFrame shape: {test_df.shape}\")\n",
    "\n",
    "# 3. Split conversations\n",
    "train_conversations = [conv for conv in filtered_conversations if int(conv['id']) in train_ids]\n",
    "test_conversations = [conv for conv in filtered_conversations if int(conv['id']) in test_ids]\n",
    "\n",
    "print(f\"Training conversations count: {len(train_conversations)}\")\n",
    "print(f\"Test conversations count: {len(test_conversations)}\")\n",
    "\n",
    "# 4. Save the split DataFrames\n",
    "train_df.to_csv(train_df_path, index=False)\n",
    "test_df.to_csv(test_df_path, index=False)\n",
    "print(\"Training and test DataFrames saved successfully!\")\n",
    "\n",
    "# 5. Save the conversations\n",
    "with open(train_json_path, 'w') as f:\n",
    "    json.dump(train_conversations, f)\n",
    "\n",
    "with open(test_json_path, 'w') as f:\n",
    "    json.dump(test_conversations, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_id_set = downsampled_df['study_id'].tolist()\n",
    "with open('../../data/modelling_ids.json', 'w') as json_file:\n",
    "    json.dump(list(study_id_set), json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Represent Test conversations as single QA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conversations: 100%|██████████| 5993/5993 [00:00<00:00, 88070.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total QA pairs extracted: 23972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def transform_to_qa(test_conversations):\n",
    "    qa_list = []  # List to hold all QA pairs\n",
    "\n",
    "    # Iterate over each conversation item with a progress bar\n",
    "    for item in tqdm(test_conversations, desc='Processing conversations'):\n",
    "        convs = item.get('conversations', [])\n",
    "        conv_id = item.get('id')\n",
    "        image = item.get('image')\n",
    "        diagnosis = item.get('diagnosis')\n",
    "\n",
    "        # Initialize index\n",
    "        i = 0\n",
    "        while i < len(convs):\n",
    "            # Check if the current turn is from 'human'\n",
    "            if convs[i].get('from') == 'human':\n",
    "                human_msg = convs[i].get('value', '').strip()\n",
    "\n",
    "                # Check if the next turn exists and is from 'gpt'\n",
    "                if i + 1 < len(convs) and convs[i + 1].get('from') == 'gpt':\n",
    "                    gpt_msg = convs[i + 1].get('value', '').strip()\n",
    "\n",
    "                    # Append the QA pair to the list, maintaining 'from' and 'value'\n",
    "                    qa_list.append({\n",
    "                        'id': conv_id,\n",
    "                        'image': image,\n",
    "                        'conversations': [\n",
    "                            {\n",
    "                                'from': 'human',\n",
    "                                'value': human_msg\n",
    "                            },\n",
    "                            {\n",
    "                                'from': 'gpt',\n",
    "                                'value': gpt_msg\n",
    "                            }\n",
    "                        ],\n",
    "                        #'diagnosis': diagnosis\n",
    "                    })\n",
    "\n",
    "                    # Move to the next pair\n",
    "                    i += 2\n",
    "                else:\n",
    "                    # If there's no corresponding 'gpt' response, skip to next\n",
    "                    i += 1\n",
    "            else:\n",
    "                # If the current turn is not from 'human', skip to next\n",
    "                i += 1\n",
    "    return qa_list\n",
    "\n",
    "qa_list = transform_to_qa(test_conversations)\n",
    "\n",
    "print(f\"Total QA pairs extracted: {len(qa_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 40569673,\n",
       "  'image': '40569673.jpeg',\n",
       "  'conversations': [{'from': 'human',\n",
       "    'value': \"<image>\\nAllergies: \\nNo Known Allergies / Adverse Drug Reactions\\n \\nChief Complaint:\\nsyncope\\n \\nMajor Surgical or Invasive Procedure:\\nNone\\n\\n \\nHistory of Present Illness:\\n___ yo F PMH dementia, CHF, shoulder pain, new anemia, presenting \\nw/ syncope. Nursing home staff noticed increased swelling in BLE \\nwith labored breathing, syncopal episode while sitting in chair \\nwith LOC for few minutes, negative headstrike. Pt was \\ndiaphoretic, nauseous, and c/o chest/neck tightness/pain. BNP \\nfound to be ___ at OSH, Received 40 Lasix, 324 asa pta. 99% on \\n3L O2 via NC. Upon arrival to ___ ED pt is alert to self only. \\nDenies any SOB at this time, normal breathing pattern, skin \\nappears pale. 98% on 2L. Endorses dizziness, nausea. Denies \\nchest pain, v/d, and fevers/chills.\\n\\nPt was recently admitted to ___ ___ for CHF \\nexacerbation and RLL PNA. Pt was treated with diuresis and \\nantibiotics at the time. Hemoglobin at d/c 9.8. Cr @ d/c 0.9.\\n\\nPast Medical History:\\n1. CARDIAC RISK FACTORS   \\n=======================\\n- ESSENTIAL (PRIMARY) HYPERTENSION  \\n- HYPERLIPIDEMIA, UNSPECIFIED  \\n- HYPERTENSION NOS \\n\\n2. CARDIAC HISTORY  \\n===================\\nSocial History:\\n___\\nFamily History:\\nFather - MI in ___\\nBrother - MI in ___\\nMother - MI in ___\\n \\nMedications on Admission:\\nThe Preadmission Medication list is accurate and complete.\\n\\n Review this patient's ECG. Check for any evidence of ST elevation.\"},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'No, there are no signs of ST elevation on the ECG.'}]},\n",
       " {'id': 40569673,\n",
       "  'image': '40569673.jpeg',\n",
       "  'conversations': [{'from': 'human',\n",
       "    'value': \"Examine the patient's ECG and ascertain if there are signs of ST depression or T wave inversion.\"},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'The ECG does not exhibit ST depression or T-wave inversion.'}]},\n",
       " {'id': 40569673,\n",
       "  'image': '40569673.jpeg',\n",
       "  'conversations': [{'from': 'human',\n",
       "    'value': '. Is it advisable to proceed with a troponin test?'},\n",
       "   {'from': 'gpt', 'value': 'Yes, we should arrange for a troponin test.'}]},\n",
       " {'id': 40569673,\n",
       "  'image': '40569673.jpeg',\n",
       "  'conversations': [{'from': 'human',\n",
       "    'value': '. Troponin test result: 1.09,cTropnT > 0.10 ng/mL suggests Acute MI.. Might this patient be having a heart attack?'},\n",
       "   {'from': 'gpt',\n",
       "    'value': 'The patient is currently experiencing a heart attack. Since there is no ST elevation, I diagnose a Non-ST Elevation Myocardial Infarction (NSTEMI).'}]},\n",
       " {'id': 41958322,\n",
       "  'image': '41958322.jpeg',\n",
       "  'conversations': [{'from': 'human',\n",
       "    'value': \"<image>\\nAllergies: \\nNo Known Allergies / Adverse Drug Reactions\\n \\nChief Complaint:\\nVomiting, diarrhea\\n \\nMajor Surgical or Invasive Procedure:\\nR IJ placement\\nL percutaneous nephrostomy ___ ___\\n\\n \\nHistory of Present Illness:\\n___ w/ PMH hypothyroidism, HLD, recurrent UTIs who presented to \\n___ complaining of 4-day hx generalized malaise, \\nvomiting/diarrhea. Per patient symptoms began with \\nnausea/vomiting with loose stools ___ per day. She had \\ndecreased appetite and poor PO intake. Additionally she has had \\nchills and developed subjective fever yesterday prompting \\npresentation to the ED. Patient denies history of dysuria, \\nhematuria, increased urinary frequency. She does endorse \\nleft-sided low back pain that wraps around to her lower abdomen \\nthat improved with home Tylenol. Otherwise denies abdominal \\npain, distension. No cough/SOB, sputum production, CP, \\npalpitations. Reports her urine is always foul-smelling. \\n\\nPer her daughter's report, when EMS arrived at patient's house, \\nHR was up to 170s, systolic BP in ___. HR was reportedly \\nirregular and patient was in Afib for approx. ___ hrs. At ___ \\n___, patient received 4L IVF, vanc/zosyn x1 and partial dose \\nof flagyl. UA was reportedly dirty with CXR negative for PNA. \\nGiven concern for urosepsis and persistent hypotension despite \\nIVF resuscitation, patient was started on peripheral \\nphenylephrine and transferred to ___ for further care. \\n\\nPast Medical History:\\nHYPOTHYROIDISM\\nHYPERLIPIDEMIA\\nS/p Partial Thyroidectomy due to benign thyroid nodule ___\\nL Rotator cuff tear \\nC-section\\n\\n \\nSocial History:\\n___\\nFamily History:\\nNo family history of heart disease.\\n \\nMedications on Admission:\\nThe Preadmission Medication list is accurate and complete.\\n1. Simvastatin 10 mg PO QPM \\n2. Levothyroxine Sodium 75 mcg PO DAILY \\n3. Vitamin D 1000 UNIT PO DAILY \\n\\n \\n\\n Review this patient's ECG. Determine if there are signs of ST elevation.\"},\n",
       "   {'from': 'gpt', 'value': 'No, ST elevation is not indicated on the ECG.'}]}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/test_conversations_single_qa.json', 'w') as f:\n",
    "    json.dump(qa_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conversations: 100%|██████████| 23972/23972 [00:00<00:00, 26812.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total QA pairs extracted: 95888\n"
     ]
    }
   ],
   "source": [
    "qa_list = transform_to_qa(train_conversations)\n",
    "print(f\"Total QA pairs extracted: {len(qa_list)}\")\n",
    "\n",
    "with open('../../data/train_conversations_single_qa.json', 'w') as f:\n",
    "    json.dump(qa_list, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
