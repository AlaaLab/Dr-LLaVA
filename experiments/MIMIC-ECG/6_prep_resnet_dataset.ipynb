{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ecg_df = pd.read_csv('../../data/mimic-acute-mi.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ecg_df = all_ecg_df[~all_ecg_df['simple_note'].isna()]\n",
    "all_ecg_df = all_ecg_df[(all_ecg_df['troponin'] & ~all_ecg_df['valuenum'].isnull() & ~all_ecg_df['comments'].isnull()) | (all_ecg_df['troponin'] == 0)]\n",
    "display(all_ecg_df.head())\n",
    "print(all_ecg_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_df = pd.read_csv('../../data/mimic-acute-mi_modelling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ecg_df = all_ecg_df[~all_ecg_df.study_id.isin(downsampled_df.study_id)]\n",
    "print(all_ecg_df.STEMI.sum())\n",
    "print(all_ecg_df.NSTEMI.sum())\n",
    "print()\n",
    "\n",
    "print(all_ecg_df.st_elevation.sum())\n",
    "print(all_ecg_df.st_depression.sum())\n",
    "print(all_ecg_df.t_wave.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sample rows for a specific condition\n",
    "def sample_condition(df, condition, count, sampled_indices):\n",
    "    condition_df = df[condition & ~df.index.isin(sampled_indices)]\n",
    "    if len(condition_df) < count:\n",
    "        raise ValueError(f\"Not enough rows to sample for condition: {condition}\")\n",
    "    sampled = condition_df.sample(n=count, random_state=RANDOM_STATE)\n",
    "    sampled_dfs.append(sampled)\n",
    "    return sampled.index\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Define the minimum required counts for each ECG category\n",
    "required_counts = {\n",
    "    'STEMI': 2000,\n",
    "    'NSTEMI': 3200,\n",
    "    'ST_elevation': 4000,\n",
    "    'ST_depression': 600,\n",
    "    'T_wave_inversion': 200\n",
    "}\n",
    "\n",
    "# Create a copy of the original dataframe to work with\n",
    "df = all_ecg_df.copy()\n",
    "\n",
    "# Initialize an empty list to store the sampled dataframes\n",
    "sampled_dfs = []\n",
    "\n",
    "# Initialize a set to keep track of sampled indices to avoid duplication where necessary\n",
    "sampled_indices = set()\n",
    "\n",
    "# Sample STEMI ECGs\n",
    "sampled_indices.update(sample_condition(\n",
    "    df,\n",
    "    df['STEMI'] == 1,\n",
    "    required_counts['STEMI'],\n",
    "    sampled_indices\n",
    "))\n",
    "\n",
    "# Sample NSTEMI ECGs\n",
    "sampled_indices.update(sample_condition(\n",
    "    df,\n",
    "    df['NSTEMI'] == 1,\n",
    "    required_counts['NSTEMI'],\n",
    "    sampled_indices\n",
    "))\n",
    "\n",
    "# Sample ST-elevation ECGs\n",
    "sampled_indices.update(sample_condition(\n",
    "    df,\n",
    "    df['st_elevation'] == 1,\n",
    "    required_counts['ST_elevation'],\n",
    "    sampled_indices\n",
    "))\n",
    "\n",
    "# Sample ST-depression ECGs\n",
    "sampled_indices.update(sample_condition(\n",
    "    df,\n",
    "    df['st_depression'] == 1,\n",
    "    required_counts['ST_depression'],\n",
    "    sampled_indices\n",
    "))\n",
    "\n",
    "# Sample T-wave inversion ECGs\n",
    "sampled_indices.update(sample_condition(\n",
    "    df,\n",
    "    df['t_wave'] == 1,\n",
    "    required_counts['T_wave_inversion'],\n",
    "    sampled_indices\n",
    "))\n",
    "\n",
    "# Concatenate all sampled dataframes\n",
    "downsampled_df = pd.concat(sampled_dfs)\n",
    "\n",
    "# Calculate the remaining number of rows to reach 30,000\n",
    "remaining_rows = 30000 - len(downsampled_df)\n",
    "\n",
    "# Check if there are enough remaining rows to sample\n",
    "if remaining_rows > 0:\n",
    "    # Exclude already sampled indices\n",
    "    remaining_df = df[~df.index.isin(sampled_indices)]\n",
    "    \n",
    "    if len(remaining_df) < remaining_rows:\n",
    "        raise ValueError(\"Not enough remaining rows to reach 30,000 after sampling required categories.\")\n",
    "    \n",
    "    # Sample the remaining rows\n",
    "    remaining_sampled = remaining_df.sample(n=remaining_rows, random_state=RANDOM_STATE)\n",
    "    downsampled_df = pd.concat([downsampled_df, remaining_sampled])\n",
    "\n",
    "# Shuffle the final dataframe\n",
    "downsampled_df = downsampled_df.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "# Optional: Verify the counts\n",
    "print(\"Downsampled DataFrame Shape:\", downsampled_df.shape)\n",
    "print(\"STEMI Count:\", downsampled_df['STEMI'].sum())\n",
    "print(\"NSTEMI Count:\", downsampled_df['NSTEMI'].sum())\n",
    "print(\"ST Elevation Count:\", (downsampled_df['st_elevation'] == 1).sum())\n",
    "print(\"ST Depression Count:\", (downsampled_df['st_depression'] == 1).sum())\n",
    "print(\"T-wave Inversion Count:\", (downsampled_df['t_wave'] == 1).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_ids = downsampled_df['study_id'].tolist()\n",
    "print(study_ids[:10])\n",
    "# Path to the image directory\n",
    "image_dir = '../../data/image_folder'\n",
    "\n",
    "# Initialize a set to store the integer filenames\n",
    "image_ids = set()\n",
    "\n",
    "# Traverse through each file in the image directory\n",
    "for filename in os.listdir(image_dir):\n",
    "    # Check if the file is a JPEG image\n",
    "    if filename.lower().endswith('.jpeg'):\n",
    "        # Remove the '.jpeg' extension\n",
    "        name_without_ext = filename[:-5]\n",
    "        try:\n",
    "            # Convert the filename to an integer and add to the set\n",
    "            file_id = int(name_without_ext)\n",
    "            image_ids.add(file_id)\n",
    "        except ValueError:\n",
    "            # If the filename is not an integer, skip it\n",
    "            print(f\"Skipping file with non-integer name: {filename}\")\n",
    "\n",
    "# Convert study_ids to a set for efficient lookup\n",
    "study_ids_set = set(study_ids)\n",
    "\n",
    "# Find the intersection of study_ids and image_ids\n",
    "matching_ids = study_ids_set.intersection(image_ids)\n",
    "\n",
    "# Calculate the number of matching IDs\n",
    "num_matching = len(matching_ids)\n",
    "total_study_ids = len(study_ids)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Total number of study_ids: {total_study_ids}\")\n",
    "print(f\"Number of study_ids with corresponding image files: {num_matching}\")\n",
    "print(f\"Percentage matched: { (num_matching / total_study_ids) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_df = downsampled_df[downsampled_df['study_id'].isin(matching_ids)] \n",
    "print(downsampled_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_df.to_csv('../../data/mimic-acute-mi_resnet.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "RANDOM_STATE = 42\n",
    "TRAIN_RATIO = 0.8\n",
    "\n",
    "# Paths to save split DataFrames\n",
    "train_df_path = '../../data/train_reset_df.csv'\n",
    "test_df_path = '../../data/test_reset_df.csv'\n",
    "\n",
    "\n",
    "# 1. Split the study_ids\n",
    "study_ids = downsampled_df['study_id'].astype(int).unique()\n",
    "train_ids, test_ids = train_test_split(\n",
    "    study_ids,\n",
    "    test_size=1 - TRAIN_RATIO,\n",
    "    random_state=RANDOM_STATE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Training study_ids: {len(train_ids)}\")\n",
    "print(f\"Testing study_ids: {len(test_ids)}\")\n",
    "\n",
    "# 2. Create training and test DataFrames\n",
    "train_df = downsampled_df[downsampled_df['study_id'].astype(int).isin(train_ids)].reset_index(drop=True)\n",
    "test_df = downsampled_df[downsampled_df['study_id'].astype(int).isin(test_ids)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Training DataFrame shape: {train_df.shape}\")\n",
    "print(f\"Test DataFrame shape: {test_df.shape}\")\n",
    "\n",
    "# 4. Save the split DataFrames\n",
    "train_df.to_csv(train_df_path, index=False)\n",
    "test_df.to_csv(test_df_path, index=False)\n",
    "print(\"Training and test DataFrames saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dr-llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
